{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title: Data Lake Project\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project gathers the US I94 immigration data along with US airport code data and US city demographics data to create a data lake using Pyspark for future use.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import os\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "Create a data lake with Pyspark with the immigration, airport data and US city demographics by partitioning parquet files in table directories stored in AWS S3 for future use following schema-on-read semantics. The output parquet table directories could be used to analyze immigration trends focucing on immigration origins and airport destinations. The generated partitioned parquet files in table directories are:\n",
    "\n",
    "- immigration_table\n",
    "- city_table\n",
    "- airport_table\n",
    "\n",
    "Example of Future Use:\n",
    "\n",
    "- Which US airports are the most traveled to in a year? Which ones are the most traveled to in a month for a particular year? What would it be the busiest monthly prediction for the upcoming years? What would it be the busiest days for the upcoming months? Such predictions could help out CBP (U.S. Customs and Border Protection) to forecast US airports insights by relying on advanced analytics.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "I-94 Immigration Data: The SAS files come from US National Tourism and Trade Office and have a data dictionary file named \"I94_SAS_Labels_Description.SAS\". This project loads all immigration data for year 2016. All 12 files have more than 40 million records which fulfills the project requeriment to have a minimum of 1 million record in a file. This project also loads countries and visa categories from \"I94_SAS_Labels_Description.SAS\".\n",
    "\n",
    "US City Demographic Data: This data is from OpenSoft and has US city demographics data. For more information, go to\n",
    "https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "Airport Code Data: This data is a list of US airport codes, US region, and other information related to aiport data.. For more information, go to https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('capstone.cfg'))\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Setup spark session \n",
    "spark = SparkSession.builder.\\\n",
    "appName(\"Capstone Project - US Immigration Data\").\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\").\\\n",
    "config(\"spark.hadoop.fs.s3a.endpoint\", \"s3-us-west-2.amazonaws.com\") .\\\n",
    "config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']).\\\n",
    "config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']).\\\n",
    "config(\"spark.sql.broadcastTimeout\", \"36000\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://40e3b184fbf5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Capstone Project - US Immigration Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7defc4e470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/18-83510-I94-Data-2016/\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DATA = config['DATA']['OUTPUT_DATA']\n",
    "BUCKET_NAME = config['DATA']['BUCKET_NAME']\n",
    "OUTPUT_DIR = config['DATA']['OUTPUT_DIR']\n",
    "I94_FOLDER = config['DATA']['I94_FOLDER']\n",
    "print(I94_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i94_apr16_sub.sas7bdat\n",
      "i94_sep16_sub.sas7bdat\n",
      "i94_nov16_sub.sas7bdat\n",
      "i94_mar16_sub.sas7bdat\n",
      "i94_jun16_sub.sas7bdat\n",
      "i94_aug16_sub.sas7bdat\n",
      "i94_may16_sub.sas7bdat\n",
      "i94_jan16_sub.sas7bdat\n",
      "i94_oct16_sub.sas7bdat\n",
      "i94_jul16_sub.sas7bdat\n",
      "i94_feb16_sub.sas7bdat\n",
      "i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# Print all files under I94 data from year 2016\n",
    "for filename in os.listdir(I94_FOLDER):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Append function to merge dataframes\n",
    "def append_dataframes(dfa,dfb):\n",
    "    \"\"\"\n",
    "    Description: This function merges 2 dataframes\n",
    "    Arguments: dataframe (a), dataframe (b)\n",
    "    Returns: a new dataframe after union of 2 dataframes\n",
    "    \"\"\"\n",
    "    lista = dfa.columns\n",
    "    listb = dfb.columns\n",
    "    for col in listb:\n",
    "        if(col not in lista):\n",
    "            dfa = dfa.withColumn(col, F.lit(None))\n",
    "    for col in lista:\n",
    "        if(col not in listb):\n",
    "            dfb = dfb.withColumn(col, F.lit(None))\n",
    "    return dfa.unionByName(dfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load all files under I94_FOLDER config variable\n",
    "for f,filename in enumerate(os.listdir(I94_FOLDER)):\n",
    "    if f == 0:\n",
    "        df = spark.read.format('com.github.saurfang.sas.spark').load(I94_FOLDER + filename)\n",
    "        df_i94 = df\n",
    "    else:\n",
    "         df = spark.read.format('com.github.saurfang.sas.spark').load(I94_FOLDER + filename)\n",
    "         df_i94 = append_dataframes(df_i94,df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40790529\n"
     ]
    }
   ],
   "source": [
    "# Print total number of rows - it needs to be 40790529\n",
    "print(df_i94.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# Print total number of columns\n",
    "print(len(df_i94.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- validres: double (nullable = true)\n",
      " |-- delete_days: double (nullable = true)\n",
      " |-- delete_mexl: double (nullable = true)\n",
      " |-- delete_dup: double (nullable = true)\n",
      " |-- delete_visa: double (nullable = true)\n",
      " |-- delete_recdup: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of df_i94 Spark dataframe\n",
    "df_i94.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------+-----------+-----------+----------+-----------+-------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------+-----------+-----------+----------+-----------+-------------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|    null|       null|       null|      null|       null|         null|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|    null|       null|       null|      null|       null|         null|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|    null|       null|       null|      null|       null|         null|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|    null|       null|       null|      null|       null|         null|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|    null|       null|       null|      null|       null|         null|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------+-----------+-----------+----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 rows of df_i94 Spark dataframe\n",
    "df_i94.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./us-cities-demographics.csv\n"
     ]
    }
   ],
   "source": [
    "# Get US_CITY_DEMO config variable\n",
    "US_CITY_DEMO = config['DATA']['US_CITY_DEMO']\n",
    "print(US_CITY_DEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2891, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read the city demographics CSV file and print total number of rows and columns\n",
    "df_city = spark.read.csv(US_CITY_DEMO, sep = ';', header=True)\n",
    "print((df_city.count(), len(df_city.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 rows of df_city\n",
    "df_city.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of df_city\n",
    "df_city.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./airport-codes_csv.csv\n"
     ]
    }
   ],
   "source": [
    "# Get AIRPORT_CODE config variable\n",
    "AIRPORT_CODE = config['DATA']['AIRPORT_CODE']\n",
    "print(AIRPORT_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55075, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read the airport codes CSV file and print total number of rows and columns\n",
    "df_airport = spark.read.csv(AIRPORT_CODE, header=True)\n",
    "print((df_airport.count(), len(df_airport.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 rows of df_airport\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of df_airport\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./I94_SAS_Labels_Descriptions.SAS\n"
     ]
    }
   ],
   "source": [
    "# Get I94_DATA_DICT config variable\n",
    "I94_DATA_DICT = config['DATA']['I94_DATA_DICT']\n",
    "print(I94_DATA_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read SAS labels and descriptions file then remove special characters\n",
    "with open(I94_DATA_DICT) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line.replace('\"','').replace('\\n','').replace(\"'\",'').replace(' ','').replace('\\t','') for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Retrieve pair codes and names function from SAS labels and descriptions file\n",
    "def get_value_pairs(value_pairs, pairs):\n",
    "    \"\"\"\n",
    "    Description: This function retrieves the pair codes and names from SAS labels and descriptions file\n",
    "    Arguments: Empty list of codes/names, lines to get codes/names from SAS file\n",
    "    Returns: List of codes/names\n",
    "    \"\"\"\n",
    "    for pair in pairs:\n",
    "        value_code = pair.split(\"=\")[0]\n",
    "        value_name = pair.split(\"=\")[1]\n",
    "        value_pairs.append((value_code, value_name))\n",
    "    return value_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: double (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n",
      "+------------+---------------+\n",
      "|country_code|country_name   |\n",
      "+------------+---------------+\n",
      "|236.0       |AFGHANISTAN    |\n",
      "|101.0       |ALBANIA        |\n",
      "|316.0       |ALGERIA        |\n",
      "|102.0       |ANDORRA        |\n",
      "|324.0       |ANGOLA         |\n",
      "|529.0       |ANGUILLA       |\n",
      "|518.0       |ANTIGUA-BARBUDA|\n",
      "|687.0       |ARGENTINA      |\n",
      "|151.0       |ARMENIA        |\n",
      "|532.0       |ARUBA          |\n",
      "|438.0       |AUSTRALIA      |\n",
      "|103.0       |AUSTRIA        |\n",
      "|152.0       |AZERBAIJAN     |\n",
      "|512.0       |BAHAMAS        |\n",
      "|298.0       |BAHRAIN        |\n",
      "|274.0       |BANGLADESH     |\n",
      "|513.0       |BARBADOS       |\n",
      "|104.0       |BELGIUM        |\n",
      "|581.0       |BELIZE         |\n",
      "|386.0       |BENIN          |\n",
      "+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get list of all country codes and names from SAS labels and descriptions file\n",
    "country_value_pairs = list()\n",
    "countries = lines[10:245]\n",
    "country_value_pairs = get_value_pairs(country_value_pairs, countries)\n",
    "country_schema = StructType([\n",
    "        StructField(\"country_code\", StringType()),\n",
    "        StructField(\"country_name\", StringType())])\n",
    "df_countries = spark.createDataFrame(data=country_value_pairs,schema=country_schema)\n",
    "df_countries_new = df_countries.withColumn('country_code', df_countries['country_code'].cast(DoubleType()))\n",
    "df_countries_new.printSchema()\n",
    "df_countries_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visa_code: double (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n",
      "+---------+---------+\n",
      "|visa_code|visa_type|\n",
      "+---------+---------+\n",
      "|1.0      |Business |\n",
      "|2.0      |Pleasure |\n",
      "|3.0      |Student  |\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get list of all visa codes and names from SAS labels and descriptions file\n",
    "visas = lines[1046:1049]\n",
    "visa_value_pairs = list()\n",
    "visa_value_pairs = get_value_pairs(visa_value_pairs, visas)\n",
    "visa_schema = StructType([\n",
    "        StructField(\"visa_code\", StringType()),\n",
    "        StructField(\"visa_type\", StringType())])\n",
    "df_visas = spark.createDataFrame(data=visa_value_pairs,schema=visa_schema)\n",
    "df_visas_new = df_visas.withColumn('visa_code', df_visas['visa_code'].cast(DoubleType()))\n",
    "df_visas_new.printSchema()\n",
    "df_visas_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join df_countries and df_visas with i_94 \n",
    "df_i94_new = df_i94.join(df_countries_new).where(df_i94['i94res'] == df_countries_new['country_code'])\\\n",
    ".join(df_visas_new).where(df_i94['i94visa'] == df_visas_new['visa_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+-----------+-----------+----------+-----------+-------------+------------+------------+---------+---------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|country_code|country_name|visa_code|visa_type|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+-----------+-----------+----------+-----------+-------------+------------+------------+---------+---------+\n",
      "|  84584.0|2016.0|   4.0| 299.0| 299.0|    POO|20545.0|    1.0|     PA|20546.0|  33.0|    1.0|  1.0|20160401|    null| null|      O|      I|   null|      M| 1983.0|09302016|  null|  null|     KE| 9.251100203E10|00085|      B1|    null|       null|       null|      null|       null|         null|       299.0|    MONGOLIA|      1.0| Business|\n",
      "|1350298.0|2016.0|  11.0| 209.0| 209.0|    DAL|20766.0|    1.0|     OH|20768.0|  46.0|    1.0|  1.0|20161108|    null| null|      O|      O|   null|      M| 1970.0|02052017|  null|  null|     JL|6.7845351933E10|00012|      WB|    null|       null|       null|      null|       null|         null|       209.0|       JAPAN|      1.0| Business|\n",
      "|  84585.0|2016.0|   4.0| 299.0| 299.0|    SEA|20545.0|    1.0|     PA|20559.0|  33.0|    1.0|  1.0|20160401|    null| null|      O|      N|   null|      M| 1983.0|09302016|  null|  null|     KE| 9.251108453E10|00085|      B1|    null|       null|       null|      null|       null|         null|       299.0|    MONGOLIA|      1.0| Business|\n",
      "|3725524.0|2016.0|   1.0| 209.0| 209.0|    DAL|20471.0|    1.0|     TN|20475.0|  52.0|    1.0|  1.0|20160120|    null| null|      G|      O|   null|      M| 1964.0|04162016|     M|  null|     JL|5.1720842333E10|00012|      WB|    null|       null|       null|      null|       null|         null|       209.0|       JAPAN|      1.0| Business|\n",
      "|  84586.0|2016.0|   4.0| 299.0| 299.0|    SEA|20545.0|    1.0|     PA|20602.0|  33.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     KE| 9.251109553E10|00085|      B1|    null|       null|       null|      null|       null|         null|       299.0|    MONGOLIA|      1.0| Business|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+-----------+-----------+----------+-----------+-------------+------------+------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 records of consolidated i_94 Spark dataframe\n",
    "df_i94_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38129404\n"
     ]
    }
   ],
   "source": [
    "# Print total number of rows\n",
    "print(df_i94_new.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# Print total number of columns\n",
    "print(len(df_i94_new.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "- Remove duplicates from the 3 Spark dataframes: *immigration_table*, *city_table* and *airport_table*.\n",
    "- Remove irrelevant columns from immigration_data: cicid, visapost, occup, dtadfile, entdepa, entdepd, dtaddto, validres, delete_days, delete_mexl, delete_dup, delete_visa and delete_recdup.\n",
    "- Remove duplicate columns from immigration_data: \"i94visa\", \"i94res\".\n",
    "- Replace all column names that have a space with underscore from city_table.\n",
    "- Remove every row that has any null value from city_table and airport_table.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "1. Create each of the 3 dataframe tables using distinct.\n",
    "2. Remove columns that are not relevant from immigration_table.\n",
    "3. Remove duplicate columns from immigration_data: \"i94visa\", \"i94res\".\n",
    "4. Replace all column names that have a space with underscore from city_table.\n",
    "5. Remove every row that has any null value from city_table and airport_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38129329 23\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows from the 3 Spark dataframes: immigration_table, city_table and airport_table\n",
    "# along with columns that are not relevant\n",
    "immigration_table = df_i94_new.select(\"i94yr\", \"i94mon\", \"i94cit\",\"i94port\", \"arrdate\", \"i94mode\", \"i94addr\", \"depdate\", \"i94bir\", \"count\",\"matflag\",\"biryear\",\"dtaddto\",\"gender\",\"insnum\",\"airline\",\"admnum\",\"fltno\",\"visatype\",\"country_code\",\"country_name\",\"visa_code\",\"visa_type\").distinct()\n",
    "# Print total number of rows and columns of immigration table\n",
    "print(immigration_table.count(), len(immigration_table.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- country_code: double (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- visa_code: double (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of immigration table\n",
    "immigration_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2875, 12)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows of city table and replace all column names \n",
    "# that have a space with underscore from city_table. Drop any null values of city table.\n",
    "city_table = df_city.select(col(\"City\").alias(\"city\"), col(\"State\").alias(\"state\"), col(\"Median Age\").alias(\"median_age\"), \\\n",
    "        col(\"Male Population\").alias(\"male_population\"), col(\"Female Population\").alias(\"female_population\"), \\\n",
    "        col(\"Total Population\").alias(\"total_population\"), col(\"Number of Veterans\").alias(\"number_of_veterans\"), \\\n",
    "        col(\"Foreign-born\").alias(\"foreign_born\"), col(\"Average Household Size\").alias(\"average_household_size\"), \\\n",
    "        col(\"State Code\").alias(\"state_code\"), col(\"Race\").alias(\"race\"), \"count\").na.drop().distinct()\n",
    "# Print total number of rows and columns of city table \n",
    "print((city_table.count(), len(city_table.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: string (nullable = true)\n",
      " |-- male_population: string (nullable = true)\n",
      " |-- female_population: string (nullable = true)\n",
      " |-- total_population: string (nullable = true)\n",
      " |-- number_of_veterans: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      " |-- average_household_size: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of city table\n",
    "city_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2746, 12)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows and drop any null values of airport table\n",
    "airport_table = df_airport.na.drop().distinct()\n",
    "# Print total number of rows and columns of airport table\n",
    "print((airport_table.count(), len(airport_table.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of airport dimension table\n",
    "airport_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "Generate partitioned parquet files in table directories:\n",
    "\n",
    "| Table Name      | Description        | Partition By  | \n",
    "| --- | --- | --- | \n",
    "| immigration_table | Fact table that has US i94 immigration data | i94year, i94mon and i94port |\n",
    "| city_table | Dimmension table that has city demographics data | state_code |\n",
    "| airport_table | Dimmension table that has US airport data | iata_code |\n",
    "\n",
    "The immigration_table has been partitioned by year, month and airport code for better performance on aggregration queries. This could be even more helpful when loading i94 files from many years. The aiport_table has been aggregrated by iata_code (airport code) while city_table has been aggregrated by state_code. Data dictionary of data model is included below.\n",
    "\n",
    "This data model was chosen since the project intention is to create a data lake with Pyspark with the immigration, airport and US city demographics data by partitioning parquet files in table directories stored in AWS S3 for future use following schema-on-read semantics.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Load all 12 I94 immigration files, city demographics file, airport code file, country information and visa data to Spark dataframes.\n",
    "2. Join country and visa data to I94 immigration Spark dataframe.\n",
    "3. Remove duplicated records for each of the 3 Spark dataframes: immigration_table, city_table and airport_table.\n",
    "4. Remove columns that are not relevant from immigration_table.\n",
    "5. Remove every row that has any null value from city_table and airport_table.\n",
    "6. Create parquet table directories partitioned them by the columns listed above from the cleaned Spark dataframes.\n",
    "7. Create a view for each of the cleaned Spark dataframes to execute SQL queries.\n",
    "8. Execute quality checks on cleaned Spark dataframes and parquet table directories:\n",
    "    - Check if each of the 3 Spark dataframes (immigration_table, city_table and airport_table) has records.\n",
    "    - Check if each parquet directory exists in S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create parquet file from immigration_table\n",
    "immigration_table.write.partitionBy(\"i94yr\",\"i94mon\",\"i94port\").mode(\"ignore\").parquet(OUTPUT_DATA + \"immigration_table.parquet\")\n",
    "#dataFrame.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(\"temp.parquet\")\n",
    "# Create table view from immigration_table\n",
    "immigration_table.createOrReplaceTempView(\"immigration_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create parquet file from city_table\n",
    "city_table.write.partitionBy(\"state_code\").mode(\"ignore\").parquet(OUTPUT_DATA + \"city_table.parquet\")\n",
    "# Create table view from city_table\n",
    "city_table.createOrReplaceTempView(\"city_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create parquet file from airport_table\n",
    "airport_table.write.partitionBy(\"iata_code\").mode(\"ignore\").parquet(OUTPUT_DATA + \"airport_table.parquet\")\n",
    "# Create table view from airport_table\n",
    "airport_table.createOrReplaceTempView(\"airport_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "- Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "- Unit tests for the scripts to ensure they are doing the right thing\n",
    "- Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks\n",
    "\n",
    "- Check if each of the 3 Spark dataframes (immigration_table, city_table and airport_table) has records.\n",
    "- Check if each parquet directory exists in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Data quality check function to confirm that each quality check has records\n",
    "def quality_check(check_data,check_name,check_function):\n",
    "    \"\"\"\n",
    "    Description: This function runs quality checks\n",
    "    Arguments: Quality check, quality check name, check function\n",
    "    Returns: Number of records for the quality check\n",
    "    \"\"\"\n",
    "    \n",
    "    if check_function == 'file_exist':    \n",
    "        s3 = boto3.resource('s3')\n",
    "        bucket = s3.Bucket(BUCKET_NAME)\n",
    "        objs = list(bucket.objects.filter(Prefix=check_data))\n",
    "        result = len(objs)\n",
    "        if (len(objs) > 0):\n",
    "            result = 'exist'\n",
    "    elif check_function == 'count': \n",
    "        result = check_data.count() \n",
    "        \n",
    "    if result == 0:\n",
    "        print(\"FAILED DATA QUALITY CHECK for {} with zero records\".format(check_name))\n",
    "    else:\n",
    "        print(\"PASSED DATA QUALITY CHECK for {} with {} records\".format(check_name, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED DATA QUALITY CHECK for immigration_table with 38129329 records\n",
      "PASSED DATA QUALITY CHECK for city_table with 2875 records\n",
      "PASSED DATA QUALITY CHECK for airport_table with 2746 records\n"
     ]
    }
   ],
   "source": [
    "# Call quality check function to check that each of the 3 table dataframes has records\n",
    "quality_check(immigration_table,'immigration_table','count')\n",
    "quality_check(city_table,'city_table','count')\n",
    "quality_check(airport_table,'airport_table','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED DATA QUALITY CHECK for immigration_table.parquet with exist records\n",
      "PASSED DATA QUALITY CHECK for city_table.parquet with exist records\n",
      "PASSED DATA QUALITY CHECK for airport_table.parquet with exist records\n"
     ]
    }
   ],
   "source": [
    "# Call quality check function to check that each parquet directory exists\n",
    "parquet_dir_check = OUTPUT_DIR + 'immigration_table.parquet/'\n",
    "quality_check(parquet_dir_check,'immigration_table.parquet','file_exist')\n",
    "parquet_dir_check = OUTPUT_DIR + 'city_table.parquet/'\n",
    "quality_check(parquet_dir_check,'city_table.parquet','file_exist')\n",
    "parquet_dir_check = OUTPUT_DIR + 'airport_table.parquet/'\n",
    "quality_check(parquet_dir_check,'airport_table.parquet','file_exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "- **immigration_table**: It came from 12 files related i94 immigration data residing in Udacity workspace with code and visa category data.\n",
    "\n",
    "| Column Name | Description | \n",
    "| ----------- | ----------- |\n",
    "| i94yr | 4 digit year |\n",
    "| i94mon | 2 digit month |\n",
    "| i94cit | 3 digit code of country in transit |\n",
    "| ***i94port*** | 3 character code of aiport (***matches \"aita_code\" from airport_table***) |\n",
    "| arrdate | arrival date |\n",
    "| i94mode | 1 digit travel code |\n",
    "| ***i94addr*** | 2 digit state code (***matches \"state_code\" from city_table***) |\n",
    "| depdate | departure date |\n",
    "| i94bir | age in years |\n",
    "| count | count summary |\n",
    "| matflag | match of arrival and departure |\n",
    "| biryear | 4 digit of birth year |\n",
    "| dtaddto | date to stay in US |\n",
    "| gender | gender of the immigrant |\n",
    "| insnum | immigration number |\n",
    "| airline | airline code |\n",
    "| admnjum | admission number |\n",
    "| fltno | flight number |\n",
    "| visatype | type of visa |\n",
    "| country_code | 3 digit code of origin country |\n",
    "| country_name | name of origin country |\n",
    "| visa_code | 1 digit visa code |\n",
    "| visa_code | visa category |\n",
    "\n",
    " - **city_table**: It came from US cities demographics file from Udacity workspace.\n",
    "\n",
    "| Column Name | Description | \n",
    "| ----------- | ----------- |\n",
    "| i94yr | 4 digit year |\n",
    "| i94mon | 2 digit month |\n",
    "| i94cit | 3 digit code of country in transit |\n",
    "| ***i94port*** | 3 character code of aiport (***matches \"aita_code\" from airport_table***) |\n",
    "| arrdate | arrival date |\n",
    "| i94mode | 1 digit travel code |\n",
    "| ***i94addr*** | 2 digit state code (***matches \"state_code\" from city_table***) |\n",
    "| depdate | departure date |\n",
    "| i94bir | age in years |\n",
    "| count | count summary |\n",
    "| matflag | match of arrival and departure |\n",
    "| biryear | 4 digit of birth year |\n",
    "| dtaddto | date to stay in US |\n",
    "| gender | gender of the immigrant |\n",
    "| insnum | immigration number |\n",
    "| airline | airline code |\n",
    "| admnjum | admission number |\n",
    "| fltno | flight number |\n",
    "| visatype | type of visa |\n",
    "| country_code | 3 digit code of origin country |\n",
    "| country_name | name of origin country |\n",
    "| visa_code | 1 digit visa code |\n",
    "| visa_code | visa category |\n",
    " \n",
    " | Column Name | Description | \n",
    " | ----------- | ----------- |\n",
    " | city | name of city |\n",
    " | state | name of state |\n",
    " | median_age | median age of city |\n",
    " | male_population | population of males |\n",
    " | female_population | population of females |\n",
    " | total_population | total population of city |\n",
    " | number_of_veterans | number of veterans of city |\n",
    " | foreign_born | foreign born |\n",
    " | average_household_size | size of average household |\n",
    " | ***state_code*** | 2 digit state code (***matches \"i94addr\" from immigration_table***) |\n",
    " | race | race |\n",
    " | count | count |\n",
    " \n",
    " - **airport_table**: It came from US airport codes file from Udacity workspace.\n",
    " \n",
    " | Column Name | Description | \n",
    " | ----------- | ----------- |\n",
    " | ident | airport identification |\n",
    " | type | airport type |\n",
    " | name | airport name | \n",
    " | elevation_ft | feet elevation |\n",
    " | continent | continent |\n",
    " | iso_country | country code |\n",
    " | iso_region | 2 digit country - 2 digit state |\n",
    " | municipality | municipality |\n",
    " | gps code | GPS code |\n",
    " | ***iata_code*** | airport code (***matches \"i94port\" from immigration_table***) |\n",
    " | local_code | local code |\n",
    " | coordinates | coordinates |\n",
    "\n",
    "| Column Name | Description | \n",
    "| ----------- | ----------- |\n",
    "| i94yr | 4 digit year |\n",
    "| i94mon | 2 digit month |\n",
    "| i94cit | 3 digit code of country in transit |\n",
    "| ***i94port*** | 3 character code of aiport (***matches \"aita_code\" from airport_table***) |\n",
    "| arrdate | arrival date |\n",
    "| i94mode | 1 digit travel code |\n",
    "| ***i94addr*** | 2 digit state code (***matches \"state_code\" from city_table***) |\n",
    "| depdate | departure date |\n",
    "| i94bir | age in years |\n",
    "| count | count summary |\n",
    "| matflag | match of arrival and departure |\n",
    "| biryear | 4 digit of birth year |\n",
    "| dtaddto | date to stay in US |\n",
    "| gender | gender of the immigrant |\n",
    "| insnum | immigration number |\n",
    "| airline | airline code |\n",
    "| admnjum | admission number |\n",
    "| fltno | flight number |\n",
    "| visatype | type of visa |\n",
    "| country_code | 3 digit code of origin country |\n",
    "| country_name | name of origin country |\n",
    "| visa_code | 1 digit visa code |\n",
    "| visa_code | visa category |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Example of Future Use\n",
    "\n",
    "- Which US airports are the most traveled to in a year? Which ones are the most traveled to in a month from a particular year? What would it be the busiest monthly prediction for the upcoming years? What would it be the busiest days for the upcoming months? Such predictions could help out CBP (U.S. Customs and Border Protection) to forecast US airports insights by relying on advanced analytics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|port_code|immigrant_visits|\n",
      "+---------+----------------+\n",
      "|      NYC|         6494862|\n",
      "|      MIA|         4888188|\n",
      "|      LOS|         4256277|\n",
      "|      HHW|         2248676|\n",
      "|      SFR|         2227517|\n",
      "|      NEW|         1837420|\n",
      "|      CHI|         1642329|\n",
      "|      ORL|         1549953|\n",
      "|      AGA|         1337858|\n",
      "|      ATL|         1044287|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 busiest US airports in 2016\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            i.i94port AS port_code, \n",
    "            COUNT(*) AS immigrant_visits\n",
    "        FROM immigration_view i \n",
    "        WHERE i.i94yr = 2016\n",
    "        GROUP BY port_code\n",
    "        ORDER BY immigrant_visits DESC\n",
    "        LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+\n",
      "|month_code|port_code|immigrant_visits|\n",
      "+----------+---------+----------------+\n",
      "|       7.0|      NYC|          711925|\n",
      "|       8.0|      NYC|          682500|\n",
      "|      10.0|      NYC|          644660|\n",
      "|       9.0|      NYC|          620518|\n",
      "|       5.0|      NYC|          607725|\n",
      "|       6.0|      NYC|          603737|\n",
      "|      12.0|      MIA|          541342|\n",
      "|       3.0|      NYC|          508969|\n",
      "|      12.0|      NYC|          480137|\n",
      "|       4.0|      NYC|          473333|\n",
      "|       7.0|      MIA|          460132|\n",
      "|       7.0|      LOS|          457300|\n",
      "|       8.0|      LOS|          439210|\n",
      "|       9.0|      LOS|          433155|\n",
      "|      11.0|      NYC|          427773|\n",
      "|       8.0|      MIA|          421012|\n",
      "|      10.0|      MIA|          420437|\n",
      "|      11.0|      MIA|          418982|\n",
      "|       3.0|      MIA|          415674|\n",
      "|       1.0|      MIA|          404042|\n",
      "+----------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 20 busiest US airports in a month from 2016\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            i.i94mon AS month_code,\n",
    "            i.i94port AS port_code,  \n",
    "            COUNT(*) AS immigrant_visits\n",
    "        FROM immigration_view i \n",
    "        WHERE i.i94yr = 2016\n",
    "        GROUP BY month_code, port_code\n",
    "        ORDER BY immigrant_visits DESC\n",
    "        LIMIT 20\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "- This project uses Python version 3.6.3 with Pyspark.\n",
    "- Apache Spark (Pyspark) is used due to its parallelism and scalability when handling large datasets.\n",
    "- Parquet output files in columnar format are used for aggregation.\n",
    "\n",
    "##### Propose how often the data should be updated and why.\n",
    "- The I94 data is aggregated on year, month and airport code basis. Therefore updating data on a monthly base is ideal.\n",
    "\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "##### The data was increased by 100x.\n",
    " - Use AWS EMR & Spark to process the data. This can be executed after creating a AWS EMR cluster and running capstone.py with the spark-submit command.\n",
    " \n",
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " - A schematized data warehouse should be created based on the parquet output files using Apache Airflow to schedule a Spark job to run on a daily basis. In case of any issue arises, email notification can be setup to notify a team to fix any errors. \n",
    " \n",
    "##### The database needed to be accessed by 100+ people.\n",
    " - Use AWS Redshift to load the parquet output files. Then a schematized data warehouse should be created based on the parquet output files. Apache Cassandra could also be used due to its scalability and good read performance for large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
